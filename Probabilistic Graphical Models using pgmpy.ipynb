{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:968f74608d1d215afcea509ff61ba97d530faaf4c8884716a11dd7a39810f235"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image, Math"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Introduction\n",
      "Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data. Such algorithms operate by building a model from example inputs and using that to make predictions or decisions, rather than following strictly static program instructions. Probabilistic Graphical Models (PGMs) are an important method in the machine learning field. In this notebook, we will discuss how you can use pgmpy to understand and use them.\n",
      "\n",
      "Consider the problem of [classification](http://en.wikipedia.org/wiki/Statistical_classification) that is encountered quite frequently. Classification involves assigning a class label to data points.\n",
      "\n",
      "There are multiple ways to solve this problem:  \n",
      "1. We could find a function which can directly map an input value to it's class label. \n",
      "2. We can find the probability distributions over the variables and then use this distribution to answer queries about the new data point.\n",
      "\n",
      "In Probabilistic Graphical Models we try to do predictions using the second method. \n",
      "The most obvious way to do this would be to compute a Joint Probability Distribution over all these variables and then marginalize and reduce over these according to our new data point to get the probabilities of classes. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Let's take an example of class where we want to predict things about students. So, we have some historical data of students and for each student we have 5 features: Difficulty (D), Intelligence (I), SAT (S), Grade (G) and Recommendation Letter (L). For simplicity, we will also consider that each of these features can have only two values. So, difficulty can take two values easy and hard, Intelligence can take smart and dumb and so on.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's now generate some random data. \n",
      "# Now let's try to compute the Joint Probability Distribution over these variables."
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "We now have a Joint Distribution $ P(D, I, S, G, L) $. Now let's say we have a new student and we want to predict what recommendation letter will he get and also say that we know that he is intelligent, the course was easy, his SAT score is good, and also he got a good grade. So, basically we want to compute $ P(L | d^0, i^1, s^1, g^1) $. From chain rule of probability we know that:\n",
      "$$ P(D, I, S, G, L) = P(L | D, I, S, G) * P(D, I, S, G) $$\n",
      "$$ P(D, I, S, G, L) = P(D | I, S, G, L) * P(I | S, G, L) * P(S | G, L) * P(G | L) * P(L) $$\n",
      "$$ P(L | d^0, i^0, s^1, g^1) = \\frac {P(d^0, i^1, s^1, g^1, L)} {P(d^0, i^1, s^1, g^1)} $$\n",
      "\n",
      "Since we know the joint distribution, $ P(D, I, S, G, L) $ we can now easily compute both the terms $ P(d^0, i^1, s^1, g^1, L) $ and $ P(d^0, i^1, s^1, g^1) $ and using these we will be able to compute the $ P(L | d^0, i^0, s^1, g^1) $.\n",
      "\n",
      "#### TODO: Show the computed values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The way we solved the problem above we had to compute the whole joint distribution to answer queries over this model. Also, we can see that the size of the table is exponential to the number of variables involved. So, in the case of large models this method becomes infeasible."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Probabilistic Graphical Models (PGM)\n",
      "\n",
      "Probabilistic Graphical Model is a way of compactly representing Joint Probability distribution over random variables by exploiting the independence conditions of the variables. PGMs also provide us methods to easily and efficiently compute conditional probabilities over joint distributions. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### How is PGM different from other techniques?\n",
      "\n",
      "The distinctive thing about PGM is that it provides a very intuitive and natural approach for modelling complex problems along with maintaining control over the computational costs."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "There are two main types of modes in PGM:\n",
      "1. Bayesian Models: Bayesian Models are used mainly when we have causal relationship between the variables\n",
      "2. Markov Models: When non causal relationship."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "In the student example that we saw, we mostly have a causal relationship between variables. So, if a student is intelligent it implies that he should get good grades, getting good grades implies that he will get a better recommendation. Let's try to create a graph of dependencies."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=\"files/student.png\" width=\"800\" height=\"600\" align=\"middle\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This network structure implies many independence conditions between the variables. Any variable in the network is independent of its non descendents given its parents. This is known as local independencies in Bayesian Networks.\n",
      "$$ (X \\perp NonDesc_X | Pa_X) $$\n",
      "\n",
      "Now let's consider global independencies in the network. Connections between variables in a directed graph can only be the following four ways:\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/connections_directed_graphs.png\" height=\"600\" width=\"800\" align=\"middle\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the case of causal, evidential and common cause connections A and C are dependent if B is not observed but if B is observed then the become independent. Whereas in the case of common evidence A and C are independent if B is not observed but become dependent if B is observed.\n",
      "\n",
      "So in the case of our student example we can now get many dependence assertions over the network like $ L \\perp D | G $, $ L \\perp I | G $, $ D \\perp I $, $ D \\not\\perp I | G $ etc."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Active Trail\n",
      "Two nodes in a Bayesian Network are called to be on an active trail if change in one of the variables affects the other. An active trail is formed if the trail between the variables only have causal, evidential or common cause relations and if a common evidence relation is present then the common evidence is observed.The common evidence relation is also commonly known as V structure."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now coming back to our original joint distribution distribution:\n",
      "$$ P(D, I, G, S, L) = P(D | I, G, S, L) * P(I | G, S, L) * P(G | S, L) * P(S | L) * P(L) $$\n",
      "\n",
      "So using these independence properties in a Bayesian Network we can reduce this to:\n",
      "$$ P(D, I, G, S, L) = P(D) * P(I) * P(G | D, I) * P(S | I) * P(L | G) $$\n",
      "\n",
      "We can also notice that all the terms in this are the probabilites of each the variables given their parents in the network. These terms are known as the conditional Probability distributions of the variables. So basically to compute the Joint Distribuiton we can compute the product of CPDs of all the variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Another example using pgmpy \n",
      "\n",
      "Let's see an example for predicting the price of a house. For simplicity we will consider that the price of the house depends only on Area, Location, Furnishing, Crime Rate and Distance from the airport. And also we will consider that all of these are discrete variables."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Our raw data would look something like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "raw_data = np.random.randint(low=0, high=2, size=(1000, 6))\n",
      "print(raw_data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 0 0 1 0 1]\n",
        " [0 1 0 0 0 1]\n",
        " [1 1 1 1 0 0]\n",
        " ..., \n",
        " [0 0 1 1 0 0]\n",
        " [0 0 1 0 0 0]\n",
        " [1 1 1 1 0 1]]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Probabilistic Graphical Models keeps the representation of Joint Distribution simple by exploiting the dependencies in the variables. And these dependencies are represented using a graph. Now let's create a model for the interaction of our variables using intuition:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"files/housing_price_small.png\" height=\"600\" width=\"800\" align = \"middle\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"files/housing_price.png\" height=\"600\" width=\"800\" align = \"middle\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "For each node/variable in the network we assign a conditional probability distribution to it. The conditional probability distribution represents the probability of the variable when its parents are observed. So now using our raw data let's assign CPDs to each of these nodes. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pgmpy.models import BayesianModel\n",
      "import pandas as pd\n",
      "data = pd.DataFrame(raw_data, columns=['A', 'C', 'D', 'L', 'F', 'P'])\n",
      "data_train = data[: int(data.shape[0] * 0.75)]\n",
      "model = BayesianModel([('F', 'P'), ('A', 'P'), ('L', 'P'), ('C', 'L'), ('D', 'L')])\n",
      "\n",
      "model.is_active_trail('A', 'D')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.is_active_trail('D', 'P')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.fit(data_train)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### What does fit does ?\n",
      "The fit method adds a Conditional Probability Distribution (CPD) to each of the node in our model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src='files/housing_price_with_CPD.png' height=\"600\" width=\"800\" align = \"middle\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.get_cpds()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[<pgmpy.factors.CPD.TabularCPD at 0x7feec235aba8>,\n",
        " <pgmpy.factors.CPD.TabularCPD at 0x7feec235a7f0>,\n",
        " <pgmpy.factors.CPD.TabularCPD at 0x7feec2301160>,\n",
        " <pgmpy.factors.CPD.TabularCPD at 0x7feec2301048>,\n",
        " <pgmpy.factors.CPD.TabularCPD at 0x7feec235afd0>,\n",
        " <pgmpy.factors.CPD.TabularCPD at 0x7feea74dfd68>]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.get_cpds('P')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "html": [
        "<table><caption>TabularCPD for <b>P</b></caption><tr><td><b>A</b></td><td colspan=4>A_0</td><td colspan=4>A_1</td></tr><tr><td><b>L</b></td><td colspan=2>L_0</td><td colspan=2>L_1</td><td colspan=2>L_0</td><td colspan=2>L_1</td></tr><tr><td><b>F</b></td><td colspan=1>F_0</td><td colspan=1>F_1</td><td colspan=1>F_0</td><td colspan=1>F_1</td><td colspan=1>F_0</td><td colspan=1>F_1</td><td colspan=1>F_0</td><td colspan=1>F_1</td></tr><tr><td><b>P_0</b></td><td>0.4783</td><td>0.5795</td><td>0.5556</td><td>0.5435</td><td>0.3922</td><td>0.4860</td><td>0.5714</td><td>0.4556</td></tr><tr><td><b>P_1</b></td><td>0.5217</td><td>0.4205</td><td>0.4444</td><td>0.4565</td><td>0.6078</td><td>0.5140</td><td>0.4286</td><td>0.5444</td></tr></table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "<pgmpy.factors.CPD.TabularCPD at 0x7feec2301160>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "But the data that we have for training might be baised so with pgmpy we also have the option to assign your own Conditional Probability Distributions. Let's say the probability of getting an unfurnished home is equal to getting a furnished house. Let's adjust the values according to this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pgmpy.factors import TabularCPD\n",
      "f_cpd = TabularCPD('F', 2, [[0.5], [0.5]])\n",
      "\n",
      "model.remove_cpds('F')\n",
      "model.add_cpds(f_cpd)\n",
      "\n",
      "model.check_model()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Inference"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Now let's try to do some reasoning on our model to verify if our intuitution for the model was correct or not."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pgmpy.inference import VariableElimination\n",
      "model = VariableElimination(model)\n",
      "# Returns a probability distribution over variables A and B.\n",
      "model.query(variables=['A', 'P'])\n",
      "print(model.query(variables=['P'], conditions={'L': {0}, 'F': {0}, 'A': {0}}))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'VariableElimination' object has no attribute 'nodes'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-12-2e20952062f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpgmpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariableElimination\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariableElimination\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Returns a probability distribution over variables A and B.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'P'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'P'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconditions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'L'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'F'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'A'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/ankur/pgmpy/pgmpy/inference/base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcardinality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'VariableElimination' object has no attribute 'nodes'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "```\n",
      "P          phi(A)  \n",
      "------------------\n",
      "P_0        0.96\n",
      "P_1        0.04\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "If you think about prediction about new values from this model, it is basically the same what we have been doing here. We basically ask questions about the probability of some variable giving conditions for other variables. Also we can account for missing values with just leaving it blank."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data = data[0.75 * data.shape[0] : data.shape[0]]\n",
      "test_data.drop('P', axis=1, inplace=True)\n",
      "model.predict(test_data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
